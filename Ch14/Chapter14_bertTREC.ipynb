{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "This lab is adapted from the Tensorflow tutorial for  text classification with BERT\n",
    "\n",
    "https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=EqL7ihkN_862\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 The TensorFlow Hub Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ6SNYq_tVVC"
   },
   "source": [
    "# Classify text with BERT\n",
    "\n",
    "Steps\n",
    "\n",
    "- Load a BERT model from TensorFlow Hub\n",
    "- Install data\n",
    "- Build a model by combining BERT with a classifier\n",
    "- Fine-tune BERT to create a model based on the TREC data\n",
    "- Save the model and use it to classify texts\n",
    "- Check for weak classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHBpLPuQdmK"
   },
   "source": [
    "## About BERT\n",
    "The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after. This is the reason for the name: Bidirectional Encoder Representations from Transformers. \n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks. The example we will show here is one of the standard BERT models with fine-tuning on the movie review corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCjmX4zTCkRK"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-YbjCkzw0yU"
   },
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "# Uncomment if you need to install this\n",
    "#!pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w_XlxN1IsRJ"
   },
   "source": [
    "We will use the AdamW optimizer, which is currently the most commonly used bert optimizer, from [tensorflow/models](https://github.com/tensorflow/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-P1ZOA0FkVJ"
   },
   "outputs": [],
   "source": [
    "#!pip install -q tf-models-official==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnvd4mrtPHHV"
   },
   "source": [
    "***\n",
    "### Install the trec dataset\n",
    "We will use the trec dataset\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the total number of text files in the dataset and what the classes are\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "training_ds = tf.keras.utils.text_dataset_from_directory('trec_processed/training')\n",
    "\n",
    "class_names = training_ds.class_names\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IwI_2bcIeX8"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 200\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'trec_processed/training',\n",
    "    batch_size = batch_size,\n",
    "    validation_split = 0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'trec_processed/training',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'trec_processed/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = {}\n",
    "for class_name in class_names:\n",
    "    files_count = train_ds.list_files('trec_processed/training/' + class_name + '/*.txt')\n",
    "    files_length = files_count.cardinality().numpy()\n",
    "    category_count = {class_name:files_length}\n",
    "    files_dict.update(category_count)\n",
    "    \n",
    "# Sort the categories, largest first\n",
    "from collections import OrderedDict\n",
    "sorted_files_dict = sorted(files_dict.items(),\n",
    "key=lambda t: t[1], reverse=True)\n",
    "print(sorted_files_dict)\n",
    "\n",
    "# Convert to Pandas series\n",
    "pd_files_dict = pd.Series(dict(sorted_files_dict))\n",
    "\n",
    "# Setting figure, ax into variables\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# plot\n",
    "all_plot = sns.barplot(x=pd_files_dict.index,\n",
    "y = pd_files_dict.values, ax=ax, palette = \"Set2\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "x_train = np.concatenate([x for x, y in train_ds], axis = 0)\n",
    "\n",
    "y_val = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "x_val = np.concatenate([x for x, y in val_ds], axis = 0)\n",
    "\n",
    "y_test = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "x_test = np.concatenate([x for x, y in test_ds], axis = 0)\n",
    "\n",
    "validation_data = x_val,y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Check a few examples of data and labels\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in test_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "***\n",
    "## Loading models from TensorFlow Hub\n",
    "\n",
    "We will use one of the smaller BERT models in order for it to run in a reasonable amount of time on a desktop computer. This is slightly bigger than the model we used in Chapter 11.\n",
    "* \"small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "* There are 4 hidden layers (that is, Transformer blocks), with a hidden size of 512\n",
    "* A=8 Attention heads\n",
    "This model was trained on Wikipedia and BooksCorpus.\n",
    "There are many larger and smaller models that can be downloaded from the TensorFlow hub.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "y8_ctG55-uTX"
   },
   "outputs": [],
   "source": [
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WrcxxTRDdHi"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Text inputs are transformed to numeric token ids and arranged in several Tensors before being input to BERT. \n",
    "\n",
    "TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above\n",
    "\n",
    "We will load the preprocessing model into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose the fine-tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SQi-jWd_jzq"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4naBiEE_cZX"
   },
   "source": [
    "Let's try the preprocessing model on some text and see the output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9-zCzJpnuwS"
   },
   "outputs": [],
   "source": [
    "test_text = ['sure is a great movie. i like it']\n",
    "print(test_text)\n",
    "\n",
    "text_preprocessed = bert_preprocess_model(test_text)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqL7ihkN_862"
   },
   "source": [
    "As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (`input_words_id`, `input_mask` and `input_type_ids`).\n",
    "\n",
    "Since this text preprocessor is a TensorFlow model, It can be included in your model directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "## Using the BERT model\n",
    "\n",
    "Before putting BERT into our model, let's take a look at its outputs. Load it from TF Hub and see the returned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXxYpK8ixL34"
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OoF9mebuSZc"
   },
   "outputs": [],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm61jDrezAll"
   },
   "source": [
    "The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\n",
    "\n",
    "- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`. You can think of this as an embedding for the entire text.\n",
    "- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of this as a contextual embedding for every token in the text.\n",
    "- `encoder_outputs` are the intermediate activations of the `L` Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the i-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`.\n",
    "\n",
    "For the fine-tuning we are going to use the `pooled_output` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "Here we create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer. The parameter to the Dropout layer can be increased to make the model more robust. We are now working with a categorical classification problem (six classes), rather than a binary classification problem (two classes). Two changes that are needed in the model definition for the categorical task are in the final layer, which has six outputs, corresponding to the six classes, and a softmax activation function, as opposed to the sigmoid activation function that we used for binary problems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aksj743St9ga"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(6, activation = tf.keras.activations.softmax, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs4yhFraBuGQ"
   },
   "source": [
    "Let's check that the model runs with the output of the preprocessing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGMF8AZcB2Zy"
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(test_text))\n",
    "print(tf.keras.activations.softmax(bert_raw_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTUzNV2JE2G3"
   },
   "source": [
    "The output is meaningless, because the model has not been trained yet but it verifies that the model runs with the preprocessing.\n",
    "\n",
    "Let's take a look at the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EmzyHZXKIpm"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302"
   },
   "source": [
    "***\n",
    "## Model training\n",
    "We now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpJ3xcwDT56v"
   },
   "source": [
    "### Loss function and metrics\n",
    "\n",
    "Since this is a categorical classification problem (that is, there are more than two outcomes, we'll use the \"losses.CategoricalCrossEntropy\" loss function. \n",
    "Similarly, the metric should be \"CategoricalAccuracy\".\n",
    "Cross-entropy estimates the loss by scoring the average difference between the actual and predicted probability distributions for all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWPOZE-L3AgE"
   },
   "outputs": [],
   "source": [
    "#loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "loss = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "metrics = tf.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77psrpfzbxtp"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "For fine-tuning, we'll use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). Adam is popular because it is fast and efficient.\n",
    "\n",
    "In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5). BERT generally does best with very small learning rates for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "print(steps_per_epoch)\n",
    "\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "# a linear warmup phase over the first 10%\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5 \n",
    "\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### Loading the BERT model and training\n",
    "\n",
    "Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer, and take a look at the summary.\n",
    "It's a good idea to check the model before starting a lengthy training process to make sure the model is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss = loss,\n",
    "                         metrics=metrics)\n",
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpBuV5j2cS_b"
   },
   "source": [
    "### Training\n",
    "Training time will vary depending on the complexity of the selected BERT model.\n",
    "For this model, dataset, and number of epochs, the training should take a few hours on a cpu.\n",
    "Setting \"verbose\" to 2 is provides the maximum feedback during training and can be useful to see if things are not going as expected so that the training can be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtfDFAnN_Neu"
   },
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "\n",
    "history = classifier_model.fit(x = train_ds,\n",
    "                               validation_data = val_ds,\n",
    "                               verbose = 2,\n",
    "                               epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Let's see how the model performs on the test data. Two values will be returned -- loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "### Plot the accuracy and loss over time\n",
    "\n",
    "With the `History` object returned by `model.fit()`, you can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#!matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', linestyle=\"dashed\",label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', linestyle=\"dashed\",label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier_model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in test_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')\n",
    "    prediction = classifier_model.predict(text_batch.numpy())\n",
    "    print(prediction)\n",
    "\n",
    "    #tf.math.confusion_matrix(labels=labels, predictions=predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "scores = [[],[],[],[],[],[]]\n",
    "plots = []\n",
    "#initialize a variable to hold the predictions\n",
    "for class_name, prediction in zip(class_names,predictions):\n",
    "    print(class_name)\n",
    "    classification = np.max(prediction)\n",
    "    max_index = np.argmax(prediction)\n",
    "    scores[max_index].append(classification)\n",
    "    histogram = sns.histplot(scores,bins = 50,palette = \"Set2\")\n",
    "    print(len(scores))\n",
    "    plots.append(histogram)\n",
    "\n",
    "for i in range(len(plots)):    \n",
    "    plots[i].show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for weak classes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "scores = [[],[],[],[],[],[]]\n",
    "\n",
    "for text_batch, label_batch in train_ds.take(100):\n",
    "    for i in range(160):\n",
    "        text_to_classify = [text_batch.numpy()[i]]\n",
    "        prediction = classifier_model.predict(text_to_classify)\n",
    "        classification = np.max(prediction)\n",
    "        max_index = np.argmax(prediction)\n",
    "        scores[max_index].append(classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = []\n",
    "for i in range(len(scores)):\n",
    "    print(len(scores[i]))\n",
    "    averages.append(np.average(scores[i]))\n",
    "    \n",
    "print(averages)\n",
    "\n",
    "def make_histogram(score_data,class_name):\n",
    "    sns.histplot(score_data,bins = 100)\n",
    "    plt.xlabel(\"Probability Score\")\n",
    "    plt.title(class_name)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    make_histogram(scores[i],class_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_model.predict(x_test)\n",
    "y_pred = np.where(y_pred > .5, 1,0)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "\n",
    "predicted_classes = []\n",
    "for i in range(len(y_pred)):\n",
    "    max_index = np.argmax(y_pred[i])\n",
    "    predicted_classes.append(max_index)\n",
    "# View the results as a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,f1_score,classification_report\n",
    "conf_matrix = confusion_matrix(y_test,predicted_classes,normalize=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results as a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_classes,normalize=None)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"who is gerald ford\", \"where is the washington monument\"]\n",
    "\n",
    "one_prediction = classifier_model.predict(text)\n",
    "#print(one_prediction)\n",
    "print(one_prediction[0:])\n",
    "single = one_prediction[1]\n",
    "print(single)\n",
    "for prob in single:\n",
    "    result = round(prob,4)\n",
    "    print(result)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = classifier_model.predict(x_test)\n",
    "y_pred = np.where(y_pred > .5, 1,0)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "\n",
    "predicted_classes = []\n",
    "for i in range(len(y_pred)):\n",
    "    max_index = np.argmax(y_pred[i])\n",
    "    predicted_classes.append(max_index)\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View the results as a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_classes,normalize=None)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the confusion matrix\n",
    "#%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = conf_matrix,\n",
    "                               display_labels = class_names)\n",
    "print(class_names)\n",
    "disp.plot(xticks_rotation=75,cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()\n",
    "print(classification_report(y_test, predicted_classes, target_names = class_names))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
