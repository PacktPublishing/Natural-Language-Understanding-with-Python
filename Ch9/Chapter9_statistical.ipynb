{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c59ec168-5f0d-4071-b512-4f4feb8d2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the movie review data and split into train/test sets\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "path = './movie_reviews/'\n",
    "# we will consider only the most 1000 common words\n",
    "max_tokens = 1000\n",
    "# load files -- there are 2000 files\n",
    "movie_reviews = load_files(path)\n",
    "# the names of the categories (the labels) are automatically generated from the names of the folders in path\n",
    "# 'pos' and 'neg'\n",
    "labels = movie_reviews.target_names\n",
    "\n",
    "# Split data into training and test sets\n",
    "# since this is just an example, we will omit the dev test set\n",
    "# 'movie_reviews.data' is the movie reviews\n",
    "# 'movie_reviews.target' is the categories assigned to each review\n",
    "# 'test_size = .20' is the proportion of the data that should be reserved for testing\n",
    "# 'random_state = 42' is an integer that controls the randomization of the\n",
    "# data so that the results are reproducible\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "movies_train, movies_test, sentiment_train, sentiment_test = train_test_split(movie_reviews.data,\n",
    "movie_reviews.target,test_size = 0.20,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80adddb-3c03-4791-ac65-0e96475d4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TfidfVectorizer to create the tfIdf representation of the corpus\n",
    "# the parameters are: min_df -- the percentage of documents that the word has\n",
    "# to occur in to be considered, the tokenizer to use, and the maximum\n",
    "# number of words to consider (max_features)\n",
    "vectorizer = TfidfVectorizer(min_df = .1,tokenizer = nltk.word_tokenize,max_features = max_tokens)\n",
    "\n",
    "# fit and transform the text into tfidf format, using training text\n",
    "# here is where we build the tfidf representation of the training data\n",
    "movies_train_tfidf = vectorizer.fit_transform(movies_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ee19ff-1a10-4414-b8d5-c7f7f9b04dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the naive bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Initialize the classifier and train it\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(movies_train_tfidf, sentiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be869d82-00cd-4f8c-bdac-caa97927d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dahl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find accuracy based on test set\n",
    "movies_test_tfidf = vectorizer.fit_transform(movies_test)\n",
    "# for each document in the test data, use the classifier to predict whether its sentiment is positive or negative\n",
    "sentiment_pred = classifier.predict(movies_test_tfidf)\n",
    "sklearn.metrics.accuracy_score(sentiment_test,sentiment_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecf2adf8-9da5-4368-961e-7778e419197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132  58]\n",
      " [ 86 124]]\n"
     ]
    }
   ],
   "source": [
    "# View the results as a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(sentiment_test,sentiment_pred,normalize=None)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbe7f164-2c92-47ee-b1f1-942582a8bc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 1000, 'pos': 1000}\n"
     ]
    }
   ],
   "source": [
    "# SVM classification\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# the directory root will be wherever the movie review data is located\n",
    "# modify the path as appropriate for your system\n",
    "directory_root = \"./movie_reviews/\"\n",
    "movie_reviews = load_files(directory_root,\n",
    "encoding='utf-8',decode_error=\"replace\")\n",
    "# count the number of reviews in each category\n",
    "labels, counts = np.unique(movie_reviews.target,\n",
    "return_counts=True)\n",
    "# convert review_data.target_names to np array\n",
    "labels_str = np.array(movie_reviews.target_names)[labels]\n",
    "print(dict(zip(labels_str, counts)))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "movies_train, movies_test, sentiment_train, sentiment_test = train_test_split(movie_reviews.data,\n",
    "movie_reviews.target, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5716714f-9e84-4808-918a-31712ccea09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n",
      "[[153  37]\n",
      " [ 38 172]]\n"
     ]
    }
   ],
   "source": [
    "# We will work with a TF_IDF representation, as before\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Use the Pipeline function to construct a sequence of two processes\n",
    "# to run, one after the other -- the vectorizer and the classifier\n",
    "svc_tfidf = Pipeline([\n",
    "(\"tfidf_vectorizer\", TfidfVectorizer(\n",
    "stop_words = \"english\", max_features=1000)),\n",
    "(\"linear svc\", SVC(kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "model = svc_tfidf\n",
    "model.fit(movies_train, sentiment_train)\n",
    "sentiment_pred = model.predict(movies_test)\n",
    "accuracy_result = accuracy_score( sentiment_test,\n",
    "sentiment_pred)\n",
    "print(accuracy_result)\n",
    "0.8125\n",
    "# View the results as a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(sentiment_test,\n",
    "sentiment_pred,normalize=None)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "573eadbe-e142-41a7-bb7b-bf5491d3cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiclass, use the \"one vs rest\" approach\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "model = OneVsRestClassifier(SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fd060e6-3a9a-4fed-acd5-d1adef4a00be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## restaurant_search\n",
       "\n",
       "- I want to get some [lunch](meal)\n",
       "- I am searching for a [dinner](meal) spot\n",
       "- i'm looking for a place in the [north](location) of town\n",
       "- show me some [good](quality) [chinese](cuisine) restaurants in the [north](location)\n",
       "- how about a [mexican](cuisine) restaurant [downtown](location)\n",
       "- Are there any [indian](cuisine) spots near here\n",
       "- Italian restaurants on the [west side] (location)\n",
       "- looking for [German](cuisine) places in the [south](location)\n",
       "- what [Greek](cuisine) places are near [12345](location)\n",
       "- help me fine a [casual](atmosphere)[asian f_usion](cuisine) place\n",
       "- I am looking a [french](cuisine) restaurant [nearby](location)\n",
       "- I am looking for a [nice] (quality) [mexican](cuisine) or [thai](cuisine) place that's [not too expensive](price)\n",
       "- [cozy](atmosphere) [barbecue](cuisine) restaurant"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a look at the data\n",
    "from IPython.display import display, Markdown\n",
    "with open(\"examples/restaurant_search.md\", \"r\") as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5cf1685-5e57-44e7-9b6a-61d1377cde49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CRFToken(text='I', tag='PRP', entity='O', shape='X', pattern={}, dense_features=[]),\n",
       " CRFToken(text='want', tag='VBP', entity='O', shape='xxxx', pattern={}, dense_features=[]),\n",
       " CRFToken(text='to', tag='TO', entity='O', shape='xx', pattern={}, dense_features=[]),\n",
       " CRFToken(text='get', tag='VB', entity='O', shape='xxx', pattern={}, dense_features=[]),\n",
       " CRFToken(text='some', tag='DT', entity='O', shape='xxxx', pattern={}, dense_features=[]),\n",
       " CRFToken(text='lunch', tag='NN', entity='U-meal', shape='xxxx', pattern={}, dense_features=[])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare to train CRF\n",
    "import sklearn_crfsuite\n",
    "from spacy_crfsuite import read_file\n",
    "train_data = read_file(\"examples/restaurant_search.md\")\n",
    "\n",
    "import spacy\n",
    "from spacy_crfsuite.tokenizer import SpacyTokenizer\n",
    "from spacy_crfsuite.train import gold_example_to_crf_tokens\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "tokenizer = SpacyTokenizer(nlp)\n",
    "train_dataset = [gold_example_to_crf_tokens(ex, tokenizer = tokenizer)\n",
    "for ex in train_data\n",
    "]\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ceb6cdd-dc60-4851-bb84-011f477b4426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': [['low', 'title', 'upper'],\n",
       "  ['low',\n",
       "   'bias',\n",
       "   'prefix5',\n",
       "   'prefix2',\n",
       "   'suffix5',\n",
       "   'suffix3',\n",
       "   'suffix2',\n",
       "   'upper',\n",
       "   'title',\n",
       "   'digit'],\n",
       "  ['low', 'title', 'upper']],\n",
       " 'c1': 0.003,\n",
       " 'c2': 0.03}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import srsly\n",
    "\n",
    "component_config = srsly.read_json(\"examples/default-config.json\")\n",
    "component_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6b36679-1b28-4c7c-8be9-6a6181abc61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9db5f693-9262-4504-b4a7-685a7c4122c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'AttributeError'>",
     "evalue": "'CRF' object has no attribute 'keep_tempfiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy_crfsuite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CRFExtractor\n\u001b[0;32m      2\u001b[0m crf_extractor \u001b[38;5;241m=\u001b[39m CRFExtractor(\n\u001b[0;32m      3\u001b[0m component_config \u001b[38;5;241m=\u001b[39m component_config)\n\u001b[1;32m----> 5\u001b[0m rs \u001b[38;5;241m=\u001b[39m crf_extractor\u001b[38;5;241m.\u001b[39mfine_tune(train_dataset)\n\u001b[0;32m      6\u001b[0m rs \u001b[38;5;241m=\u001b[39m  crf_extractor\u001b[38;5;241m.\u001b[39mfine_tune(train_dataset, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rs\u001b[38;5;241m.\u001b[39mbest_params_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, score:\u001b[39m\u001b[38;5;124m\"\u001b[39m,rs\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[1;32mc:\\users\\dahl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\spacy_crfsuite\\crf_extractor.py:259\u001b[0m, in \u001b[0;36mCRFExtractor.fine_tune\u001b[1;34m(self, val_samples, cv, n_iter, n_jobs, random_state)\u001b[0m\n\u001b[0;32m    248\u001b[0m f1_scorer \u001b[38;5;241m=\u001b[39m make_scorer(metrics\u001b[38;5;241m.\u001b[39mflat_f1_score, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m    249\u001b[0m rs \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m    250\u001b[0m     crf,\n\u001b[0;32m    251\u001b[0m     params_space,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    258\u001b[0m )\n\u001b[1;32m--> 259\u001b[0m \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rs\n",
      "File \u001b[1;32mc:\\users\\dahl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:788\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    785\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    786\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[1;32m--> 788\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch)\n\u001b[0;32m    792\u001b[0m fit_and_score_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    793\u001b[0m     scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    794\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    800\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    801\u001b[0m )\n",
      "File \u001b[1;32mc:\\users\\dahl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py:87\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[0;32m     84\u001b[0m             )\n\u001b[0;32m     86\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m---> 87\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m new_object_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     89\u001b[0m     new_object_params[name] \u001b[38;5;241m=\u001b[39m clone(param, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\dahl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py:170\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    168\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names():\n\u001b[1;32m--> 170\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m    172\u001b[0m         deep_items \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mget_params()\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CRF' object has no attribute 'keep_tempfiles'"
     ]
    }
   ],
   "source": [
    "from spacy_crfsuite import CRFExtractor\n",
    "crf_extractor = CRFExtractor(\n",
    "component_config = component_config)\n",
    "\n",
    "rs =  crf_extractor.fine_tune(train_dataset, cv = 5,n_iter=50, random_state=42)\n",
    "\n",
    "print(\"best_params:\", rs.best_params_, \", score:\",rs.best_score_)\n",
    "crf_extractor.train(train_dataset)\n",
    "classification_report = crf_extractor.eval(train_dataset)\n",
    "print(classification_report[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f7df4-a716-4633-b70e-6d1ddd28f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\"text\": \"show some good chinese restaurants near me\"}\n",
    "tokenizer.tokenize(example, attribute=\"text\")\n",
    "crf_extractor.process(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
