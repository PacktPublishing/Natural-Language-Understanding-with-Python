{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the training data\n",
    "from nltk.corpus import movie_reviews\n",
    "corpus_words = movie_reviews.words()\n",
    "print(len(corpus_words))\n",
    "print(corpus_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cc1df-82f2-485a-8458-f5a164c2772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "words_no_punct = []\n",
    "for word in corpus_words:\n",
    "    if word.isalnum():\n",
    "        words_no_punct.append(word)\n",
    "freq = nltk.FreqDist(words_no_punct)\n",
    "#common words \n",
    "print(\"Common Words:\", freq.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac66835-6a23-450a-9f16-9516a2c9608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.plot(50,cumulative = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486a342-2e55-4765-8531-bd72da2fc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequency distribution with a log scale on the y-axis\n",
    "plt.plot(*zip(*freq.most_common(50)))\n",
    "print(\"plotted\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Counts (log scale)')\n",
    "plt.title('Frequency Distribution with a Log Scale')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.grid(True)\n",
    "\n",
    "# Set y-axis tick labels at each power of 10\n",
    "y_ticks = [10**i for i in range(int(min(freq.values())), int(max(freq.values()))+1) if 10**i <= max(freq.values())]\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "print(\"added title\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.plot(50, cumulative = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "print(len(stop_words))\n",
    "print(stop_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = stopwords.fileids()\n",
    "print('Stopwords for ', len(languages), ' languages are included in NLTK')\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stop = [w for w in words_no_punct if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_without_stopwords = nltk.FreqDist(words_stop)\n",
    "freq_without_stopwords.plot(50, cumulative = False)\n",
    "\n",
    "print(\"Common Words:\", freq_without_stopwords.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frequency_cutoff = 25\n",
    "all_fdist = nltk.FreqDist(freq_without_stopwords).most_common(frequency_cutoff)\n",
    "\n",
    "# Convert to Pandas series\n",
    "all_fdist = pd.Series(dict(all_fdist))\n",
    "\n",
    "# Create figure and axis variables and set size for the x and y axes\n",
    "fig, ax = plt.subplots(figsize=(30,20))\n",
    "\n",
    "# Seaborn plotting using Pandas attributes \n",
    "sns.set(font_scale=3)\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "# rotate the x-axis labels for better display\n",
    "plt.xticks(rotation=90);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a wordcloud without stopwords\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "frequency_cutoff = 200\n",
    "all_fdist = nltk.FreqDist(freq_without_stopwords).most_common(frequency_cutoff)\n",
    "all_fdist = pd.Series(dict(all_fdist))\n",
    "\n",
    "long_words = dict([(m, n) for m, n in all_fdist.items() if len(m) > 2])\n",
    "wordcloud = WordCloud(width=1600, height=800,colormap=\"tab10\",background_color=\"white\").generate_from_frequencies(long_words)\n",
    "plt.figure( figsize=(20,15))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect the words in the positive and negative reviews\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "corpus_neg_words = movie_reviews.words(categories=\"neg\")\n",
    "corpus_pos_words = movie_reviews.words(categories=\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to remove punctuation and stopwords\n",
    "def clean_corpus(corpus):\n",
    "    cleaned_corpus = []\n",
    "    for word in corpus:\n",
    "        if word.isalnum() and not word in stop_words:\n",
    "            cleaned_corpus.append(word)\n",
    "    return(cleaned_corpus)\n",
    "\n",
    "def plot_freq_dist(freq_dist):\n",
    "    frequency_cutoff = 50\n",
    "    all_fdist = nltk.FreqDist(freq_dist).most_common(frequency_cutoff)\n",
    "    # Convert to Pandas series\n",
    "    all_fdist = pd.Series(dict(all_fdist))\n",
    "    # Create figure and axis variables and set size for the x and y axes\n",
    "    fig, ax = plt.subplots(figsize=(30,20))\n",
    "    # Seaborn plotting using Pandas attributes \n",
    "    sns.set(font_scale=3)\n",
    "    all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "    # rotate the x-axis labels \n",
    "    plt.xticks(rotation=90);\n",
    "    plt.show()\n",
    "\n",
    "negative_words = clean_corpus(corpus_neg_words)\n",
    "positive_words = clean_corpus(corpus_pos_words)\n",
    "\n",
    "neg_freq = nltk.FreqDist(negative_words)\n",
    "pos_freq = nltk.FreqDist(positive_words)\n",
    "\n",
    "plot_freq_dist(neg_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# remove punctuation and stopwords\n",
    "def clean_corpus(corpus):\n",
    "    cleaned_corpus = []\n",
    "    for word in corpus:\n",
    "        if word.isalnum() and not word in stop_words:\n",
    "            cleaned_corpus.append(word)\n",
    "    return(cleaned_corpus)\n",
    "\n",
    "# show a word cloud, given a frequency distribution\n",
    "def plot_freq_dist(freq_dist):\n",
    "    frequency_cutoff = 50    \n",
    "    long_words = dict([(m, n) for m, n in freq_dist.items() if len(m) > 2])\n",
    "    wordcloud = WordCloud(width=1600, height=800,colormap=\"tab10\",background_color=\"white\").generate_from_frequencies(long_words)\n",
    "    plt.figure( figsize=(20,15))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "corpus_neg_words = movie_reviews.words(categories=\"neg\")\n",
    "corpus_pos_words = movie_reviews.words(categories=\"pos\")\n",
    "negative_words = clean_corpus(corpus_neg_words)\n",
    "positive_words = clean_corpus(corpus_pos_words)\n",
    "neg_freq = nltk.FreqDist(negative_words)\n",
    "pos_freq = nltk.FreqDist(positive_words)\n",
    "\n",
    "plot_freq_dist(pos_freq)\n",
    "plot_freq_dist(neg_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram frequencies\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "frequency_cutoff = 25\n",
    "\n",
    "# collect the words from the corpus\n",
    "corpus_words = movie_reviews.words()\n",
    "\n",
    "# remove punctuation and stopwords\n",
    "cleaned_corpus = clean_corpus(corpus_words)\n",
    "\n",
    "# collect the bigrams in the corpus\n",
    "bigrams = ngrams(cleaned_corpus,2)\n",
    "\n",
    "# make a list from the bigrams\n",
    "list_bigrams = list(bigrams)\n",
    "\n",
    "# put together the bigrams into a single string\n",
    "consolidated_bigrams = []\n",
    "for bigram in list_bigrams:\n",
    "    consolidated_bigram = bigram[0] + \" \" + bigram[1]\n",
    "    consolidated_bigrams.append(consolidated_bigram)\n",
    "    \n",
    "# make a frequency distribution from the bigrams\n",
    "\n",
    "freq_bigrams = nltk.FreqDist(consolidated_bigrams).most_common(frequency_cutoff)\n",
    "# Convert to a Pandas series\n",
    "all_fdist = pd.Series(dict(freq_bigrams))\n",
    "\n",
    "# set figure and axis variables and set sizes for the x and y axes\n",
    "fig, ax = plt.subplots(figsize=(50,40))\n",
    "\n",
    "# create a bar graph using Seaborn\n",
    "sns.set(font_scale=2)\n",
    "# display the bigrams on the y-axis and the counts on the x-axis\n",
    "all_plot = sns.barplot(x=all_fdist.values, y=all_fdist.index, ax=ax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4ac56-4939-4736-b75b-d34f4dd96bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing named entities\n",
    "text =\"Apple announced a new iPhone in New York\"\n",
    "\n",
    "doc = nlp(text)\n",
    "# note that you need to run this in a Jupyter notebook to see the named entities\n",
    "displacy.render(doc, jupyter = True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the BoW \n",
    "import random\n",
    "\n",
    "#import the training data\n",
    "from nltk.corpus import movie_reviews\n",
    "corpus_words = movie_reviews.words()\n",
    "\n",
    "# remove punctuation and stopwords\n",
    "cleaned_corpus = clean_corpus(corpus_words)\n",
    "\n",
    "all_words = nltk.FreqDist(w for w in cleaned_corpus)\n",
    "max_words = 1000\n",
    "word_features = list(all_words)[:max_words]\n",
    "\n",
    "def document_features(document):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        if word in document:\n",
    "            features[word] = 1\n",
    "        else:\n",
    "            features[word] = 0\n",
    "    return features\n",
    "\n",
    "# make a list of documents\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "# collect features, that is, words that occur in a document\n",
    "featuresets = [(document_features(document), category) for (document,category) in documents]\n",
    "\n",
    "#remove categories for display\n",
    "docnumber = 0\n",
    "new_featuresets = {}\n",
    "for featureset in featuresets:\n",
    "    new_featureset = featureset[0]\n",
    "    new_featuresets[docnumber]= new_featureset\n",
    "    docnumber += 1\n",
    "    \n",
    "# display the words that occur in the first 10 documents, the bag of words\n",
    "df_featuresets = pd.DataFrame.from_dict(data = new_featuresets, orient = 'index', columns = word_features)\n",
    "df_featuresets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster the BoW with kmeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "true_k = 2\n",
    "\n",
    "# truncatedSVD for reducing dimensions to 2 for display\n",
    "truncatedSVD = TruncatedSVD(n_components = 2)\n",
    "X_2D = truncatedSVD.fit_transform(df_featuresets)\n",
    "\n",
    "kmeans = KMeans(n_clusters = true_k, \n",
    "               init='k-means++', \n",
    "               max_iter=100, # Maximum iterations\n",
    "               n_init=10)  # Number of times to run the k-means algorithm \n",
    "\n",
    "result = kmeans.fit(X_2D)\n",
    "labels = result.labels_\n",
    "\n",
    "cm = plt.get_cmap('Accent')\n",
    "\n",
    "# plot clusters in different colors\n",
    "for cluster in range(true_k):\n",
    "    current_color = cm(1.*cluster/(true_k))\n",
    "    plt.scatter(X_2D[labels == cluster, 0], X_2D[labels == cluster, 1],\n",
    "            color = current_color, label='cluster ' + str(cluster))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams['font.size'] = '12'\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407accb7-3e62-4bab-bbb6-df0244ed0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count and display the number of documents in which a word occurs\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download corpus if needed\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Load the corpus\n",
    "corpus = nltk.corpus.movie_reviews\n",
    "\n",
    "# Define the word to search for\n",
    "bad_word_to_search = \"bad\"\n",
    "good_word_to_search = \"good\"\n",
    "\n",
    "# Initialize the document counts\n",
    "pos_count_bad = 0\n",
    "neg_count_bad = 0\n",
    "pos_count_good = 0\n",
    "neg_count_good = 0\n",
    "\n",
    "# Iterate through the corpus documents\n",
    "for fileid in corpus.fileids():\n",
    "    # Load the document\n",
    "    doc_words = corpus.words(fileid)\n",
    "    # Check if the word is in the document\n",
    "    if bad_word_to_search in doc_words:\n",
    "        # Increment the appropriate document count\n",
    "        if fileid.startswith(\"pos\"):\n",
    "            pos_count_bad += 1\n",
    "        elif fileid.startswith(\"neg\"):\n",
    "            neg_count_bad += 1\n",
    "    if good_word_to_search in doc_words:\n",
    "        # Increment the appropriate document count\n",
    "        if fileid.startswith(\"pos\"):\n",
    "            pos_count_good += 1\n",
    "        elif fileid.startswith(\"neg\"):\n",
    "            neg_count_good += 1\n",
    "\n",
    "# Plot the result\n",
    "labels = ['Pos/\"good\"', 'Neg/\"good\"', 'Pos/\"bad\"', 'Neg/\"bad\"']\n",
    "values = [pos_count_good, neg_count_good,pos_count_bad,neg_count_bad]\n",
    "print(values)\n",
    "# Define custom colors for the bars\n",
    "colors = ['#008fd5', '#fc4f30', '#e5ae37', '#6d904f']\n",
    "\n",
    "# Create a column chart with custom colors\n",
    "plt.bar(labels, values, color=colors)\n",
    "\n",
    "plt.title(f\"Occurrences of 'good' and 'bad' in Positive and Negative Reviews\",fontsize=24)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylabel('Count',fontsize=20)\n",
    "plt.xlabel('Values',fontsize=20)          \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-commercial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
